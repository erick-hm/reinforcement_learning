{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "049d368b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a577b5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cufflinks as cf\n",
    "import plotly.offline\n",
    "cf.go_offline()\n",
    "cf.set_config_file(world_readable=True, theme=\"white\")\n",
    "\n",
    "from MAB.bandits import GaussianBanditGame, GaussianBandit, BernoulliBandit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1988a1",
   "metadata": {},
   "source": [
    "## 1. A Gaussian bandit game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc0809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "slotA = GaussianBandit(5,3)\n",
    "slotB = GaussianBandit(6,2)\n",
    "slotC = GaussianBandit(1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e22581",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = GaussianBanditGame([slotA, slotB, slotC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a23df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.user_play()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd91208",
   "metadata": {},
   "source": [
    "## 2. Online Advertising Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6557b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "adA = BernoulliBandit(0.004)\n",
    "adB = BernoulliBandit(0.016)\n",
    "adC = BernoulliBandit(0.02)\n",
    "adD = BernoulliBandit(0.028)\n",
    "adE = BernoulliBandit(0.031)\n",
    "ads = [adA, adB, adC, adD, adE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b99732",
   "metadata": {},
   "source": [
    "### 2.1 Strategy 1: A/B/n testing\n",
    "\n",
    "This is an exploration strategy used to determine which action should be taken by directly comparing actions. An experiment is run and at the end of the experiment, the results are compared for each action.\n",
    "\n",
    "This can be seen as a baseline strategy for solving the problem.\n",
    "\n",
    "Suppose you select an action $a$ for the $i$th time, for which you get reward $R_i$. The average reward observed prior to the $n^{th}$ selection is $$Q_n \\equiv \\frac{R_1 + ... + R_n}{n-1}$$. We can define an update rule by factoring out the $R_n$ term and multiplying by $(n-1)/(n-1)$. This gives us\n",
    "$$Q_{n+1} = Q_n + \\frac{1}{n}(R_n - Q_n)$$\n",
    "\n",
    "This tells us that to update the expected reward at the $(n+1)^{th}$ action, we just need to add the deviation of the reward from the expected value, divided by the total number of actions taken. As we make more observations, our corrections to the expected reward will get smaller and smaller.\n",
    "\n",
    "An interesting thing to note is that this could be a limitation if the environment changes with time, in which case we may want more recent observations to have the same or more importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d0d7fa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb757e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 10_000\n",
    "n_prod = 90_000\n",
    "n_ads = len(ads)\n",
    "Q = np.zeros(n_ads) # Q, action values\n",
    "N = np.zeros(n_ads) # N, total impressions\n",
    "total_reward = 0\n",
    "avg_rewards = [] # save average rewards over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7757a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each turn, randomly select an action to take\n",
    "for i in range(n_test):\n",
    "    ad_chosen = np.random.randint(n_ads)\n",
    "    R = ads[ad_chosen].pull_lever() # observe reward\n",
    "    N[ad_chosen] += 1\n",
    "    Q[ad_chosen] += (1 / N[ad_chosen]) * (R - Q[ad_chosen])\n",
    "    total_reward += R\n",
    "    avg_reward_so_far = total_reward / (i+1)\n",
    "    avg_rewards.append(avg_reward_so_far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63516980",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ad_index = np.argmax(Q)\n",
    "print(f\"The best performing ad is ad {[\"A\",\"B\", \"C\", \"D\", \"E\"][best_ad_index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793660ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a choice as to which ad was best for the test period and use that in production\n",
    "ad_chosen = best_ad_index\n",
    "for i in range(n_prod):\n",
    "    R = ads[ad_chosen].pull_lever()\n",
    "    total_reward += R\n",
    "    avg_reward_so_far = total_reward / (n_test + i + 1)\n",
    "    avg_rewards.append(avg_reward_so_far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb4a67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rewards = pd.DataFrame(avg_rewards, columns=[\"A/B/n\"])\n",
    "\n",
    "df_rewards.iplot(title=f\"A/B/n Test Avg. Reward: {avg_reward_so_far:.4f}\", xTitle=\"Impressions\", yTitle=\"Avg. Reward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c520437",
   "metadata": {},
   "source": [
    "Using this strategy, we can see that after the exploration phase ends, the average reward consistently grows until it plateaus around the average for campaign E.\n",
    "\n",
    "#### Issues with A/B/n testing\n",
    "\n",
    "* It is inefficient with the samples and does not modify the experiment dynamically by learning from observations. It doesn't take advantage of any information to cull non-promising campaigns early, for example.\n",
    "* It is unable to correct a decision once it's made. If during the test period the wrong \"best\" campaign is selected, then it is fixed for the production period. It cannot adapt.\n",
    "* It cannot adapt to changes in a dynamic environment, especially so for non stationary environments.\n",
    "* The length of the test period is a hyperparameter that has a significant effect on performance and on cost\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1aaceb",
   "metadata": {},
   "source": [
    "### 2.2 Strategy 2: $\\epsilon$-Greedy Actions\n",
    "\n",
    "The $\\epsilon$-greedy approach corrects the static nature of A/B/n testing by allowing for continuous exploration.\n",
    "\n",
    "In essence, the user should always take the greedy action that gives the best reward with probability $1 - \\epsilon$. However, with probability $\\epsilon$ it should take a random action that could be sub-optimal. Typically, the value of $\\epsilon$ is kept small to exploit the knowledge developed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3763fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.2\n",
    "n_prod = 100_000\n",
    "n_ads = len(ads)\n",
    "Q = np.zeros(n_ads)\n",
    "N = np.zeros(n_ads)\n",
    "total_reward = 0\n",
    "avg_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2715d430",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_chosen = np.random.randint(n_ads)\n",
    "for i in range(n_prod):\n",
    "    R = ads[ad_chosen].pull_lever()\n",
    "    N[ad_chosen] += 1\n",
    "    Q[ad_chosen] += (1 / N[ad_chosen]) * (R - Q[ad_chosen])\n",
    "    total_reward += R\n",
    "    avg_reward_so_far = total_reward / (i + 1)\n",
    "    avg_rewards.append(avg_reward_so_far)\n",
    "\n",
    "    if np.random.uniform() <= eps:\n",
    "        ad_chosen = np.random.randint(n_ads)\n",
    "    else:\n",
    "        ad_chosen = np.argmax(Q)\n",
    "    \n",
    "df_rewards[f\"e-greedy: {eps}\"] = avg_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9a6b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_list = ['e-greedy: 0.01', 'e-greedy: 0.05', 'e-greedy: 0.1', 'e-greedy: 0.2']\n",
    "\n",
    "df_rewards[greedy_list].iplot(title=\"e-Greedy Actions\", dash=[\"solid\", \"dash\", \"dashdot\", \"dot\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c33ee8",
   "metadata": {},
   "source": [
    "We can see from the above plot that those strategies with the lowest $\\epsilon$ values perform the worst early on, as they have very little capacity for exploration and so must accumulate many samples before they can change strategy. However, in the long run, while the higher $\\epsilon$ values plateau, we can see the smaller values continuue to increase. This lends itself to the idea that a beneficial strategy would be to start with a large value for early exploration, and then dynamically decrease the value with increasing samples.\n",
    "\n",
    "#### Disadvantages\n",
    "* $\\epsilon$-greedy actions and A/B/n tests are equally inefficient and static in allocating the exploration budget. In this particular example, you would want to drop the campaigns that are clearly performing extremely poorly and use the exploration budget on the more promising options.\n",
    "* Modifying the $\\epsilon$ greedy approach introduces more hyperparameters that require tuning.\n",
    "\n",
    "#### Advantages\n",
    "* Unlike the A/B/n approach, exploration is continuous and therefore it could feasibly adapt to a dynamic environment.\n",
    "* The $\\epsilon$ greedy approach can be made better by dynamically adjusting the value of $\\epsilon$\n",
    "* The approach can be made more dynamic by increasing the importance of the more recent observations. In the update equation for the average reward $Q_{n+1}$, we could replace the factor 1/n with a constant $\\alpha$ that allows more recent observations to have greater contribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ae76e8",
   "metadata": {},
   "source": [
    "### 2.3 Strategy 3: Upper confidence bounds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb856d55",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
