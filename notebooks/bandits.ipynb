{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "049d368b",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandits - Solution Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a577b5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cufflinks as cf\n",
    "import plotly.offline\n",
    "\n",
    "cf.go_offline()\n",
    "cf.set_config_file(world_readable=True, theme=\"white\")\n",
    "\n",
    "from MAB.bandits import GaussianBanditGame, GaussianBandit, BernoulliBandit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1988a1",
   "metadata": {},
   "source": [
    "## 1. A Gaussian bandit game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc0809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "slotA = GaussianBandit(5, 3)\n",
    "slotB = GaussianBandit(6, 2)\n",
    "slotC = GaussianBandit(1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e22581",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = GaussianBanditGame([slotA, slotB, slotC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a23df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.user_play()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd91208",
   "metadata": {},
   "source": [
    "## 2. Online Advertising Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6557b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "adA = BernoulliBandit(0.004)\n",
    "adB = BernoulliBandit(0.016)\n",
    "adC = BernoulliBandit(0.02)\n",
    "adD = BernoulliBandit(0.028)\n",
    "adE = BernoulliBandit(0.031)\n",
    "ads = [adA, adB, adC, adD, adE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b99732",
   "metadata": {},
   "source": [
    "### 2.1 Strategy 1: A/B/n testing\n",
    "\n",
    "This is an exploration strategy used to determine which action should be taken by directly comparing actions. An experiment is run and at the end of the experiment, the results are compared for each action.\n",
    "\n",
    "This can be seen as a baseline strategy for solving the problem.\n",
    "\n",
    "Suppose you select an action $a$ for the $i$th time, for which you get reward $R_i$. The average reward observed prior to the $n^{th}$ selection is $$Q_n \\equiv \\frac{R_1 + ... + R_n}{n-1}$$. We can define an update rule by factoring out the $R_n$ term and multiplying by $(n-1)/(n-1)$. This gives us\n",
    "$$Q_{n+1} = Q_n + \\frac{1}{n}(R_n - Q_n)$$\n",
    "\n",
    "This tells us that to update the expected reward at the $(n+1)^{th}$ action, we just need to add the deviation of the reward from the expected value, divided by the total number of actions taken. As we make more observations, our corrections to the expected reward will get smaller and smaller.\n",
    "\n",
    "An interesting thing to note is that this could be a limitation if the environment changes with time, in which case we may want more recent observations to have the same or more importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d0d7fa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb757e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 10_000\n",
    "n_prod = 90_000\n",
    "n_ads = len(ads)\n",
    "Q = np.zeros(n_ads)  # Q, action values\n",
    "N = np.zeros(n_ads)  # N, total impressions\n",
    "total_reward = 0\n",
    "avg_rewards = []  # save average rewards over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7757a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each turn, randomly select an action to take\n",
    "for i in range(n_test):\n",
    "    ad_chosen = np.random.randint(n_ads)\n",
    "    R = ads[ad_chosen].pull_lever()  # observe reward\n",
    "    N[ad_chosen] += 1\n",
    "    Q[ad_chosen] += (1 / N[ad_chosen]) * (R - Q[ad_chosen])\n",
    "    total_reward += R\n",
    "    avg_reward_so_far = total_reward / (i + 1)\n",
    "    avg_rewards.append(avg_reward_so_far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63516980",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ad_index = np.argmax(Q)\n",
    "print(f\"The best performing ad is ad {['A', 'B', 'C', 'D', 'E'][best_ad_index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793660ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a choice as to which ad was best for the test period and use that in production\n",
    "ad_chosen = best_ad_index\n",
    "for i in range(n_prod):\n",
    "    R = ads[ad_chosen].pull_lever()\n",
    "    total_reward += R\n",
    "    avg_reward_so_far = total_reward / (n_test + i + 1)\n",
    "    avg_rewards.append(avg_reward_so_far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb4a67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rewards = pd.DataFrame(avg_rewards, columns=[\"A/B/n\"])\n",
    "\n",
    "df_rewards.iplot(\n",
    "    title=f\"A/B/n Test Avg. Reward: {avg_reward_so_far:.4f}\",\n",
    "    xTitle=\"Impressions\",\n",
    "    yTitle=\"Avg. Reward\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c520437",
   "metadata": {},
   "source": [
    "Using this strategy, we can see that after the exploration phase ends, the average reward consistently grows until it plateaus around the average for campaign E.\n",
    "\n",
    "#### Issues with A/B/n testing\n",
    "\n",
    "* It is inefficient with the samples and does not modify the experiment dynamically by learning from observations. It doesn't take advantage of any information to cull non-promising campaigns early, for example.\n",
    "* It is unable to correct a decision once it's made. If during the test period the wrong \"best\" campaign is selected, then it is fixed for the production period. It cannot adapt.\n",
    "* It cannot adapt to changes in a dynamic environment, especially so for non stationary environments.\n",
    "* The length of the test period is a hyperparameter that has a significant effect on performance and on cost\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1aaceb",
   "metadata": {},
   "source": [
    "### 2.2 Strategy 2: $\\epsilon$-Greedy Actions\n",
    "\n",
    "The $\\epsilon$-greedy approach corrects the static nature of A/B/n testing by allowing for continuous exploration.\n",
    "\n",
    "In essence, the user should always take the greedy action that gives the best reward with probability $1 - \\epsilon$. However, with probability $\\epsilon$ it should take a random action that could be sub-optimal. Typically, the value of $\\epsilon$ is kept small to exploit the knowledge developed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3763fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.01\n",
    "n_prod = 100_000\n",
    "n_ads = len(ads)\n",
    "Q = np.zeros(n_ads)\n",
    "N = np.zeros(n_ads)\n",
    "total_reward = 0\n",
    "avg_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06757b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ad_chosen = np.random.randint(n_ads)\n",
    "# for i in range(n_prod):\n",
    "#     R = ads[ad_chosen].pull_lever()\n",
    "#     N[ad_chosen] += 1\n",
    "#     Q[ad_chosen] += (1 / N[ad_chosen]) * (R - Q[ad_chosen])\n",
    "#     total_reward += R\n",
    "#     avg_reward_so_far = total_reward / (i + 1)\n",
    "#     avg_rewards.append(avg_reward_so_far)\n",
    "\n",
    "#     if np.random.uniform() <= eps:\n",
    "#         ad_chosen = np.random.randint(n_ads)\n",
    "#     else:\n",
    "#         ad_chosen = np.argmax(Q)\n",
    "\n",
    "# df_rewards[f\"e-greedy: {eps}\"] = avg_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2715d430",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rewards = []\n",
    "for eps in [0.01, 0.05, 0.1, 0.2]:\n",
    "    Q = np.zeros(n_ads)\n",
    "    N = np.zeros(n_ads)\n",
    "    total_reward = 0\n",
    "    ad_chosen = np.random.randint(n_ads)\n",
    "    for i in range(n_prod):\n",
    "        R = ads[ad_chosen].pull_lever()\n",
    "        N[ad_chosen] += 1\n",
    "        Q[ad_chosen] += (1 / N[ad_chosen]) * (R - Q[ad_chosen])\n",
    "        total_reward += R\n",
    "        avg_reward_so_far = total_reward / (i + 1)\n",
    "        avg_rewards.append(avg_reward_so_far)\n",
    "\n",
    "        if np.random.uniform() <= eps:\n",
    "            ad_chosen = np.random.randint(n_ads)\n",
    "        else:\n",
    "            ad_chosen = np.argmax(Q)\n",
    "\n",
    "    df_rewards[f\"e-greedy: {eps}\"] = avg_rewards\n",
    "    avg_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c5ff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9a6b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_list = [\"e-greedy: 0.01\", \"e-greedy: 0.05\", \"e-greedy: 0.1\", \"e-greedy: 0.2\"]\n",
    "\n",
    "df_rewards[greedy_list].iplot(\n",
    "    title=\"e-Greedy Actions\", dash=[\"solid\", \"dash\", \"dashdot\", \"dot\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c33ee8",
   "metadata": {},
   "source": [
    "We can see from the above plot that those strategies with the lowest $\\epsilon$ values perform the worst early on, as they have very little capacity for exploration and so must accumulate many samples before they can change strategy. However, in the long run, while the higher $\\epsilon$ values plateau, we can see the smaller values continuue to increase. This lends itself to the idea that a beneficial strategy would be to start with a large value for early exploration, and then dynamically decrease the value with increasing samples.\n",
    "\n",
    "#### Disadvantages\n",
    "* $\\epsilon$-greedy actions and A/B/n tests are equally inefficient and static in allocating the exploration budget. In this particular example, you would want to drop the campaigns that are clearly performing extremely poorly and use the exploration budget on the more promising options.\n",
    "* Modifying the $\\epsilon$ greedy approach introduces more hyperparameters that require tuning.\n",
    "\n",
    "#### Advantages\n",
    "* Unlike the A/B/n approach, exploration is continuous and therefore it could feasibly adapt to a dynamic environment.\n",
    "* The $\\epsilon$ greedy approach can be made better by dynamically adjusting the value of $\\epsilon$\n",
    "* The approach can be made more dynamic by increasing the importance of the more recent observations. In the update equation for the average reward $Q_{n+1}$, we could replace the factor 1/n with a constant $\\alpha$ that allows more recent observations to have greater contribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ae76e8",
   "metadata": {},
   "source": [
    "### 2.3 Strategy 3: Upper confidence bounds\n",
    "\n",
    "UCB is an exploration-exploitation strategy that at each timestep, will choose the action that has the highest potential for reward, where the potential is defined as the sum of the action value estimate and a measure of the uncertainty estimate.\n",
    "\n",
    "At time t, we can take action $$A_t = \\arg\\max_a \\left[Q_t(a) + c \\sqrt{\\frac{\\ln t}{N_t(a)}} \\right]$$\n",
    "\n",
    "This allows us to either pick an action because it has a high estimated action value (first term) or the uncertainty in the action value is high (second term).\n",
    "\n",
    "As $\\ln t$ grows with time, if the corresponding number of times an action has been taken has not increased accordingly, then it becomes more probable to take that action. The factor $c$ is a hyperparameter that allows us to control the rate of exploration and exploitation.\n",
    "\n",
    "The second term is derived from Hoeffding's inequality, which bounds the probability that the empirical mean overstimates the actual mean by some error $\\epsilon$ for any IID variables drawn from a finite bounded interval, $$P(\\bar{X} - \\mu \\leq \\epsilon) \\leq e^{-2n\\epsilon^2}$$. If we say that the probability should be bounded by some small number $\\delta$, then we can solve for:\n",
    "$$P(\\bar{X} - \\mu \\leq \\epsilon) \\leq e^{-2n\\epsilon^2} \\leq \\delta$$\n",
    "\n",
    "which gives us $$\\epsilon \\geq \\sqrt{\\frac{-\\ln \\delta}{2n}}$$\n",
    "\n",
    "Defining $\\delta = 1/t$ so that the bound decreases with time (representing a greater exploitation rate with time), we arrive at the formulation seen above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb856d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_prod = 100_000\n",
    "n_ads = len(ads)\n",
    "ad_indices = np.array(range(n_ads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10f668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rewards = []\n",
    "for c in [0.01, 0.05, 0.1, 0.2]:\n",
    "    Q = np.zeros(n_ads)\n",
    "    N = np.zeros(n_ads)\n",
    "    total_reward = 0\n",
    "    for t in range(1, n_prod + 1):\n",
    "        if any(N == 0):\n",
    "            ad_chosen = np.random.choice(ad_indices[N == 0])\n",
    "        else:\n",
    "            uncertainty = np.sqrt(np.log(t) / N)\n",
    "            ad_chosen = np.argmax(Q + (c * uncertainty))\n",
    "\n",
    "        R = ads[ad_chosen].pull_lever()\n",
    "        N[ad_chosen] += 1\n",
    "        Q[ad_chosen] += (1 / N[ad_chosen]) * (R - Q[ad_chosen])\n",
    "        total_reward += R\n",
    "        avg_reward_so_far = total_reward / t\n",
    "        avg_rewards.append(avg_reward_so_far)\n",
    "\n",
    "    df_rewards[f\"UCB, c={c}\"] = avg_rewards\n",
    "    avg_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a17b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "ucb_list = [\"UCB, c=0.01\", \"UCB, c=0.05\", \"UCB, c=0.1\", \"UCB, c=0.2\"]\n",
    "best_reward = df_rewards.loc[t - 1, ucb_list].max()\n",
    "df_rewards[ucb_list].iplot(\n",
    "    title=f\"Action Selection using UCB. Best avg. reward: {best_reward:.4f}\",\n",
    "    dash=[\"solid\", \"dash\", \"dashdot\", \"dot\"],\n",
    "    xTitle=\"Impressions\",\n",
    "    yTitle=\"Avg. Reward\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1838a6f6",
   "metadata": {},
   "source": [
    "#### Disadvantages\n",
    "* UCB can be hard to tune because the parameter $c$ has less of an intuitive value, especially when compared to the $\\epsilon$-greedy approach.\n",
    "\n",
    "#### Advantages\n",
    "* UCB systematically and dynamically allocates exploration budget to alternative campaigns that require further exploration. If an ad suddenly becomes much more popular in a changing environment, it will slowly adapt to that.\n",
    "* UCB can be further optimised for dynamic environments - at the cost of more hyperparameters. For example, we could include exponential smoothing on the Q-values to prioritise more recent observations. There are also some more effective estimations of the uncertainty component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181824c5",
   "metadata": {},
   "source": [
    "### 2.4 Thompson Posterior Sampling\n",
    "\n",
    "The goal of the Multi-Armed Bandit problem is to estimate the parameters of the reward distribution of each arm. In the context of marketing, this corresponds to choosing an ad with the greatest click-through rate. This idea of estimating the distribution's parameters fits within the Bayesian inference framework.\n",
    "\n",
    "In the ad example, for a given ad $k$, observing a click through is a Bernoulli random variable with parameter $\\theta_k$, which we are trying to estimate.\n",
    "\n",
    "Initially, we don't have a reason to believe that the parameter is high or low for a given ad. So it makes sense to assume a uniform distribution over the interval [0,1].\n",
    "\n",
    "If we dispay the ad $k$ and it results in a click, we use this as a sinal to update the probability distribution for $\\theta_k$ so that the expected value shifts towards 1. As we collect more data, the variance in the estimate for the parameter should shrink, and this is our method for balancing exploration vs exploitation.\n",
    "\n",
    "The method uses a sample from the posterior distribution of the parameter $p(\\theta_k | X)$. If the expected value of $\\theta_k$ is high, we are likely to get samples closer to 1. If the variance is high because ad $k$ has not been selected a lot, the samples will also have high variance, which allows for exploration. \n",
    "\n",
    "At a given timestep, we take a sample from each ad's posterior distribution and select the greatest sample to choose which ad to display.\n",
    "\n",
    "It is common to choose the $\\beta$ distribution as the prior due to its bounding within the region [0,1] and because it is a conjugate distribution to a Bernoulli likelihood, which ensures the posterior is also a Bernoulli distribution:\n",
    "\n",
    "$$p(\\theta_k) = \\frac{\\Gamma(\\alpha_k + \\beta_k)}{\\Gamma (\\alpha_k) \\Gamma (\\beta_k)} \\theta_k^{\\alpha_k - 1} (1 - \\theta_k)^{\\beta_k - 1}$$\n",
    "\n",
    "Using a value of $\\alpha_k = \\beta_k = 1$ allows us to implement a uniform prior. We then use the following update rule after observing reward $R_t$ after selecting ad $k$, which gives us the posterior distribution by setting:\n",
    "\n",
    "$$\\alpha_k \\to \\alpha_k + R_t$$ \n",
    "$$\\beta_k \\to \\beta_k + 1-R_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4db5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_prod = 100_000\n",
    "n_ads = len(ads)\n",
    "alphas = np.ones(n_ads)\n",
    "betas = np.ones(n_ads)\n",
    "total_reward = 0\n",
    "avg_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5701bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_prod):\n",
    "    theta_samples = [np.random.beta(alphas[k], betas[k]) for k in range(n_ads)]\n",
    "\n",
    "    ad_chosen = np.argmax(theta_samples)\n",
    "    R = ads[ad_chosen].pull_lever()\n",
    "    alphas[ad_chosen] += R\n",
    "    betas[ad_chosen] += 1 - R\n",
    "    total_reward += R\n",
    "    avg_reward_so_far = total_reward / (i + 1)\n",
    "    avg_rewards.append(avg_reward_so_far)\n",
    "\n",
    "df_rewards[\"Thompson Sampling\"] = avg_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e720a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rewards[\"Thompson Sampling\"].iplot(\n",
    "    title=f\"Thompson Sampling Avg. Reward: {avg_reward_so_far:.4f}\",\n",
    "    xTitle=\"Impressions\",\n",
    "    yTitle=\"Avg Reward\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a189f928",
   "metadata": {},
   "source": [
    "#### Advantages\n",
    "* Does not require hyperparameter tuning because we assume a uniform prior.\n",
    "* It has efficient exploration because it dynamically adapts exploration and exploitation of different outcomes, meaning fewer costly mistakes in production due to tuning of hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188f0805",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
